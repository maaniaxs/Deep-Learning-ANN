{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNO8qlUWU2gvd2Qin4xgz4x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maaniaxs/Deep-Learning/blob/main/Language_Translation_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BUILDING MODEL_ZERO\n",
        "# eng-to-spanish Translation with Keras_NLP"
      ],
      "metadata": {
        "id": "ang75PlP_F6h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# KerasNLP provides building blocks for NLP (model layers, tokenizers, metrics, etc.) and makes it convenient to construct NLP pipelines"
      ],
      "metadata": {
        "id": "UmVANYhS_F91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import keras_nlp\n",
        "import numpy as np\n",
        "import pathlib\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "#from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab"
      ],
      "metadata": {
        "id": "Rnq9s7_y_GBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras_nlp\n",
        "import keras_nlp\n",
        "from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab"
      ],
      "metadata": {
        "id": "L2Q1jJ2QpUjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_S = 64             # Let's also define our parameters/hyperparameters.\n",
        "MAX_SEQ_LENGTH = 40\n",
        "ENG_VOCAB_SIZE = 15000\n",
        "SPA_VOCAB_SIZE = 15000\n",
        "EMBED_DIM = 256\n",
        "INTERMEDIATE_DIM = 2048\n",
        "NUM_HEADS = 8"
      ],
      "metadata": {
        "id": "6Ftxv_Vuvqxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Downloading the data. We'll be working with an English-to-Spanish translation dataset provided by Anki. Let's download it\n",
        "text_file = keras.utils.get_file(\n",
        "    fname=\"spa-eng.zip\",\n",
        "    origin=\"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\",\n",
        "    extract=True )\n",
        "text_file = pathlib.Path(text_file).parent / \"spa-eng\" / \"spa.txt\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tL6PkNtb8tg5",
        "outputId": "1dd73be0-cda8-4e80-dc3c-ac0fb68ec0b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
            "2638744/2638744 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Parsing the data\n",
        "# Each line contains an English sentence and its corresponding Spanish sentence. The English sentence is the source sequence and Spanish one \n",
        "# is the target sequence. Before adding the text to a list, we convert it to lowercase\n",
        "with open(text_file) as f:\n",
        "    lines = f.read().split(\"\\n\")[:-1]\n",
        "text_pairs = []\n",
        "for line in lines:\n",
        "    eng, spa = line.split(\"\\t\")\n",
        "    eng = eng.lower()\n",
        "    spa = spa.lower()\n",
        "    text_pairs.append((eng, spa))"
      ],
      "metadata": {
        "id": "o8iV9yXh8tj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(text_pairs), text_pairs[0:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5oS1DHnQ8tm_",
        "outputId": "2b41be17-8ff3-4193-deb6-93ae1cbb5487"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(118964,\n",
              " [('go.', 've.'),\n",
              "  ('go.', 'vete.'),\n",
              "  ('go.', 'vaya.'),\n",
              "  ('go.', 'váyase.'),\n",
              "  ('hi.', 'hola.')])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for _ in range(5):\n",
        "    print(random.choice(text_pairs))     #Here's what our sentence pairs look like:"
      ],
      "metadata": {
        "id": "NNs3Vz2V8tqT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, let's split the sentence pairs into a training set, a validation set, and a test set.\n",
        "random.shuffle(text_pairs)\n",
        "num_val_samples = int(0.15 * len(text_pairs))\n",
        "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
        "train_pairs = text_pairs[:num_train_samples]\n",
        "val_pairs = text_pairs[num_train_samples : num_train_samples + num_val_samples]\n",
        "test_pairs = text_pairs[num_train_samples + num_val_samples :]\n",
        "\n",
        "print(f\"{len(text_pairs)} total pairs\")\n",
        "print(f\"{len(train_pairs)} training pairs\")\n",
        "print(f\"{len(val_pairs)} validation pairs\")\n",
        "print(f\"{len(test_pairs)} test pairs\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWwCwMTL_XUx",
        "outputId": "3ff47c68-451c-4974-9c55-165166f983d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "118964 total pairs\n",
            "83276 training pairs\n",
            "17844 validation pairs\n",
            "17844 test pairs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizing the data\n",
        "# We'll define two tokenizers - one for the source language (English), and the other for the target language (Spanish). \n",
        "# We'll be using keras_nlp.tokenizers.WordPieceTokenizer to tokenize the text. keras_nlp.tokenizers.WordPieceTokenizer takes a WordPiece \n",
        "# vocabulary and has functions for tokenizing the text, and detokenizing sequences of tokens.\n",
        "def train_word_piece(text_samples, vocab_size, reserved_tokens):\n",
        "    bert_vocab_args = dict(\n",
        "        vocab_size=vocab_size, # The target vocabulary size\n",
        "        reserved_tokens=reserved_tokens, # Reserved tokens that must be included in the vocabulary\n",
        "        bert_tokenizer_params={\"lower_case\": True},)  # Arguments for `text.BertTokenizer`\n",
        "    \n",
        "    word_piece_ds = tf.data.Dataset.from_tensor_slices(text_samples)\n",
        "    vocab = bert_vocab.bert_vocab_from_dataset( word_piece_ds.batch(1000).prefetch(2), **bert_vocab_args)\n",
        "    return vocab"
      ],
      "metadata": {
        "id": "-2PtgOQT_XYE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "b15r_RcZBRxn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# \"[PAD]\" - Padding token. Padding tokens are appended to the input sequence length when the input sequence length is shorter than the maximum sequence length.\n",
        "# \"[UNK]\" - Unknown token.\n",
        "# \"[START]\" - Token that marks the start of the input sequence.\n",
        "# \"[END]\" - Token that marks the end of the input sequence.\n",
        "reserved_tokens = [\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",
        "eng_samples = [text_pair[0] for text_pair in train_pairs]\n",
        "eng_vocab = train_word_piece(eng_samples, ENG_VOCAB_SIZE, reserved_tokens)\n",
        "spa_samples = [text_pair[1] for text_pair in train_pairs]\n",
        "spa_vocab = train_word_piece(spa_samples, SPA_VOCAB_SIZE, reserved_tokens)"
      ],
      "metadata": {
        "id": "zdU4OoMk_XbO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(spa_samples), spa_samples[:3], len(eng_vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dern-k0B9yq",
        "outputId": "06febf2c-3123-4eed-d080-3224c44108a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(83276,\n",
              " ['ella es amable con él.',\n",
              "  'mira aquel edificio.',\n",
              "  'tom pudo estar disfrazado.',\n",
              "  'el almuerzo de tom incluye un sándwich y una manzana.',\n",
              "  'el que no está satisfecho con poco, no está satisfecho con nada.',\n",
              "  'papá noel estaba parado en el jardín.'],\n",
              " 3620)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"English Tokens: \", eng_vocab[100:110])        #Let's see some tokens!\n",
        "print(\"Spanish Tokens: \", spa_vocab[100:110])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PV9Yyps_B918",
        "outputId": "b64d2a53-34af-4806-dfe0-6201a602e6e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English Tokens:  ['re', 'how', 'll', 'did', 'very', 'as', 'had', 'all', 'here', 'about']\n",
            "Spanish Tokens:  ['del', 'estaba', 'quiero', 'tengo', 'fue', 'aqui', 'casa', 'cuando', 'hacer', '##n']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, let's define the tokenizers. We will configure the tokenizers with the the vocabularies trained above\n",
        "eng_tokenizer = keras_nlp.tokenizers.WordPieceTokenizer( vocabulary=eng_vocab, lowercase=False )\n",
        "\n",
        "spa_tokenizer = keras_nlp.tokenizers.WordPieceTokenizer( vocabulary=spa_vocab, lowercase=False )"
      ],
      "metadata": {
        "id": "ALaXcaJoB95W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eng_input_ex = text_pairs[1][0]            # For Example\n",
        "eng_tokens_ex = eng_tokenizer.tokenize(eng_input_ex)\n",
        "print(\"English-sentence: \", eng_input_ex)\n",
        "print(\"Tokens: \", eng_tokens_ex)\n",
        "print(\"Text after detokenizing: \", eng_tokenizer.detokenize(eng_tokens_ex))\n",
        "print()\n",
        "spa_input_ex = text_pairs[1][1]\n",
        "spa_tokens_ex = spa_tokenizer.tokenize(spa_input_ex)\n",
        "print(\"Spanish-sentence: \", spa_input_ex)\n",
        "print(\"Tokens: \", spa_tokens_ex)\n",
        "print(\"Text after detokenizing: \", spa_tokenizer.detokenize(spa_tokens_ex))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0Y_wZm-Fy_c",
        "outputId": "af20eeab-a014-496a-cba2-ccc58ca02e8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English-sentence:  the student body is opposed to the new rules.\n",
            "Tokens:  tf.Tensor([  57  607 1291   61 2862   58   57  187  852   12], shape=(10,), dtype=int32)\n",
            "Text after detokenizing:  tf.Tensor(b'the student body is opposed to the new rules .', shape=(), dtype=string)\n",
            "\n",
            "Spanish-sentence:  el cuerpo estudiantil está en contra de las nuevas normas.\n",
            "Tokens:  tf.Tensor([  64 1632 1824 4383   73   68  509   63   87 2910 4599   15], shape=(12,), dtype=int32)\n",
            "Text after detokenizing:  tf.Tensor(b'el cuerpo estudiantil esta en contra de las nuevas normas .', shape=(), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Format datasets,     Next, we'll format our datasets.\n",
        "# At each training step, the model will seek to predict target words N+1 (and beyond) using the source sentence and the target words 0 to N.\n",
        "# 1. inputs is a dictionary with the keys encoder_inputs and decoder_inputs. encoder_inputs is the tokenized source sentence and decoder_inputs is \n",
        "#   the target sentence \"so far\", that is to say, the words 0 to N used to predict word N+1 (and beyond) in the target sentence.\n",
        "# 2. target is the target sentence offset by one step: it provides the next words in the target sentence -- what the model will try to predict.\n",
        "# 3. We will add special tokens, \"[START]\" and \"[END]\", to the input Spanish sentence after tokenizing the text. \n",
        "#   We will also pad the input to a fixed length. This can be easily done using"
      ],
      "metadata": {
        "id": "QZPglILYFzFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_batch(eng, spa):\n",
        "    batch_size = tf.shape(spa)[0]      # ENG_VOCAB_SIZE = 15000, SPA_VOCAB_SIZE = 15000,  MAX_SEQUENCE_LENGTH = 40\n",
        "    eng = eng_tokenizer(eng)\n",
        "    spa = spa_tokenizer(spa)\n",
        "    # PADDING `eng` to `MAX_SEQUENCE_LENGTH`.\n",
        "    eng_start_end_packer = keras_nlp.layers.StartEndPacker(\n",
        "        sequence_length=MAX_SEQ_LENGTH,\n",
        "        pad_value = eng_tokenizer.token_to_id(\"[PAD]\"),) #eng_tokenizer.token_to_id(\"[PAD]\") = 0\n",
        "    eng = eng_start_end_packer(eng)\n",
        "\n",
        "    # Add special tokens (`\"[START]\"` and `\"[END]\"`) to `spa` and padding it as well.\n",
        "    spa_start_end_packer = keras_nlp.layers.StartEndPacker(\n",
        "        sequence_length=MAX_SEQ_LENGTH + 1,  # 40+1\n",
        "        start_value=spa_tokenizer.token_to_id(\"[START]\"),  # eng_tokenizer.token_to_id(\"[START]\") = 2\n",
        "        end_value=spa_tokenizer.token_to_id(\"[END]\"),    # eng_tokenizer.token_to_id(\"[END]\") = 3\n",
        "        pad_value=spa_tokenizer.token_to_id(\"[PAD]\"), )   # eng_tokenizer.token_to_id(\"[PAD]\") = 0\n",
        "    spa = spa_start_end_packer(spa)\n",
        "\n",
        "    return ({ \"encoder_inputs\": eng, \"decoder_inputs\": spa[:, :-1] }, spa[:, 1:],)\n",
        "\n",
        "def make_dataset(pairs):\n",
        "    # eng_texts, spa_texts = zip(pairs)  # it's not work\n",
        "    eng_texts, spa_texts = zip(*pairs)\n",
        "    eng_texts = list(eng_texts)\n",
        "    spa_texts = list(spa_texts)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n",
        "    dataset = dataset.batch(BATCH_S)\n",
        "    dataset = dataset.map(preprocess_batch, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    return dataset.shuffle(2048).prefetch(16).cache()\n",
        "\n",
        "train_ds = make_dataset(train_pairs)\n",
        "val_ds = make_dataset(val_pairs)"
      ],
      "metadata": {
        "id": "BWyJ_sN4FzIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ZfrW3mOQl4J",
        "outputId": "dfab46ce-0c10-4fc3-9d81-c49910e7fef3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<CacheDataset element_spec=({'encoder_inputs': TensorSpec(shape=(None, 40), dtype=tf.int32, name=None), 'decoder_inputs': TensorSpec(shape=(None, 40), dtype=tf.int32, name=None)}, TensorSpec(shape=(None, 40), dtype=tf.int32, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eng_tokenizer.token_to_id(\"[END]\"), eng_tokenizer.token_to_id(\"[PAD]\"), eng_tokenizer.token_to_id(\"[START]\"), "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zb6T7flQFzWl",
        "outputId": "ff999dda-4362-4f08-c790-22ee7febae9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3, 0, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's take a quick look at the sequence shapes (we have batches of 64 pairs, and all sequences are 40 steps long):\n",
        "for inputs, targets in train_ds.take(1):\n",
        "    print(f'inputs[\"encoder_inputs\"].shape: {inputs[\"encoder_inputs\"].shape}')\n",
        "    print(f'inputs[\"decoder_inputs\"].shape: {inputs[\"decoder_inputs\"].shape}')\n",
        "    print(f\"targets.shape: {targets.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EzLbfr64B98u",
        "outputId": "10850111-59a9-44ba-905f-be2fcba6ccde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs[\"encoder_inputs\"].shape: (64, 40)\n",
            "inputs[\"decoder_inputs\"].shape: (64, 40)\n",
            "targets.shape: (64, 40)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building The Model_1\n",
        "* English to french"
      ],
      "metadata": {
        "id": "hNx_HxNFB9_m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoder\n",
        "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")     # MAX_SEQ_LENGTH = 40\n",
        "x = keras_nlp.layers.TokenAndPositionEmbedding(vocabulary_size=ENG_VOCAB_SIZE,\n",
        "    sequence_length=MAX_SEQ_LENGTH, embedding_dim=256, mask_zero=True,)(encoder_inputs)  # EMBED_DIM = 256, \n",
        "\n",
        "encoder_outputs = keras_nlp.layers.TransformerEncoder(\n",
        "    intermediate_dim=2048, num_heads=NUM_HEADS)(inputs=x)     # INTERMEDIATE_DIM = 2048, NUM_HEADS = 8 \n",
        "encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
        "\n",
        "# Decoder\n",
        "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n",
        "encoded_seq_inputs = keras.Input(shape=(None, 256), name=\"decoder_state_inputs\")\n",
        "\n",
        "x = keras_nlp.layers.TokenAndPositionEmbedding(\n",
        "    vocabulary_size=SPA_VOCAB_SIZE, sequence_length=MAX_SEQ_LENGTH,\n",
        "    embedding_dim=256, mask_zero=True,)(decoder_inputs)\n",
        "  \n",
        "x = keras_nlp.layers.TransformerDecoder(\n",
        "    intermediate_dim=2048, num_heads=NUM_HEADS )(x, encoded_seq_inputs)     #(decoder_sequence=x, encoder_sequence=encoded_seq_inputs)\n",
        "x = keras.layers.Dropout(0.3)(x)   #x = keras.layers.Dropout(0.5)(x)\n",
        "decoder_outputs = keras.layers.Dense(SPA_VOCAB_SIZE, activation=\"softmax\")(x)\n",
        "decoder = keras.Model( [decoder_inputs, encoded_seq_inputs,], decoder_outputs,)\n",
        "\n",
        "decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
        "\n",
        "transformer = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\",)"
      ],
      "metadata": {
        "id": "sosXClLk_Xdw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#transformer.summary()\n",
        "transformer.compile(\"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "transformer.fit(train_ds, epochs=2, validation_data=val_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "en9XlVRe_XhT",
        "outputId": "8e67a294-8b38-4e4b-8f6a-924758c5ff18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "1302/1302 [==============================] - 114s 82ms/step - loss: 0.9587 - accuracy: 0.4437 - val_loss: 0.7268 - val_accuracy: 0.5555\n",
            "Epoch 2/2\n",
            "1302/1302 [==============================] - 108s 83ms/step - loss: 0.6757 - accuracy: 0.5927 - val_loss: 0.5929 - val_accuracy: 0.6256\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa0b7626820>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_sequences(input_sentences):\n",
        "    batch_size = tf.shape(input_sentences)[0]\n",
        "    # Tokenize the encoder input.\n",
        "    encoder_input_tokens = eng_tokenizer(input_sentences).to_tensor(shape=(None, MAX_SEQ_LENGTH))\n",
        "    # Define a function that outputs the next token's probability given the\n",
        "    # input sequence.\n",
        "    def token_probability_fn(decoder_input_tokens):\n",
        "        return transformer([encoder_input_tokens, decoder_input_tokens])[:, -1, :]\n",
        "\n",
        "    # Set the prompt to the \"[START]\" token.\n",
        "    prompt = tf.fill((batch_size, 1), spa_tokenizer.token_to_id(\"[START]\"))\n",
        "\n",
        "    generated_tokens = keras_nlp.utils.greedy_search(\n",
        "        token_probability_fn, prompt, max_length = 40,\n",
        "        end_token_id=spa_tokenizer.token_to_id(\"[END]\"),)\n",
        "    generated_sentences = spa_tokenizer.detokenize(generated_tokens)\n",
        "    return generated_sentences\n",
        "\n",
        "test_eng_texts = [pair[0] for pair in test_pairs[:6]]\n",
        "for i in range(4):\n",
        "    input_sentence = random.choice(test_eng_texts)\n",
        "    translated = decode_sequences(tf.constant([input_sentence]))\n",
        "    translated = translated.numpy()[0].decode(\"utf-8\")\n",
        "    translated = ( translated.replace(\"[PAD]\", \"\")\n",
        "        .replace(\"[START]\", \"\")\n",
        "        .replace(\"[END]\", \"\")\n",
        "        .strip() )\n",
        "    print(f\"** Example {i} **\")\n",
        "    print(input_sentence)\n",
        "    print(translated)\n",
        "    print()"
      ],
      "metadata": {
        "id": "CNgD9Zc4v166",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b29b7dec-177b-42f4-8647-a1746616cedf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "** Example 0 **\n",
            "i don't want to go to boston with you.\n",
            "no quiero ir a boston contigo .\n",
            "\n",
            "** Example 1 **\n",
            "they own a lot of land.\n",
            "ellos soloon un monton de arumpresentan .\n",
            "\n",
            "** Example 2 **\n",
            "that's unnecessary.\n",
            "eso es despresentar .\n",
            "\n",
            "** Example 3 **\n",
            "that's unnecessary.\n",
            "eso es despresentar .\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BUILDING  MODEL_2\n"
      ],
      "metadata": {
        "id": "0Y22HhJuW43z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras_nlp"
      ],
      "metadata": {
        "id": "UXFgsCwnCeNK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib\n",
        "import numpy as np\n",
        "import random\n",
        "import string\n",
        "import re\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "import keras_nlp"
      ],
      "metadata": {
        "id": "zJgnD4oa-t_Z"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Downloading the data. We'll be working with an English-to-Spanish translation dataset provided by Anki. Let's download it\n",
        "text_file = keras.utils.get_file( fname=\"spa-eng.zip\",\n",
        "    origin=\"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\", extract=True )\n",
        "text_file = pathlib.Path(text_file).parent / \"spa-eng\" / \"spa.txt\""
      ],
      "metadata": {
        "id": "qGwPp-wY11oL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20fd638e-bd78-4c64-e5a5-39c74f7194af"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
            "2638744/2638744 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Each line contains an English sentence and its corresponding Spanish sentence. The English sentence is the source sequence and Spanish one \n",
        "#  is the target sequence.\n",
        "\n",
        "text = text_file.read_text(encoding='utf-8')\n",
        "lines = text.splitlines()\n",
        "\n",
        "eng = [l.split('\\t')[0] for l in lines]\n",
        "spa = [l.split('\\t')[1] for l in lines]\n",
        "text_pairs = [l.split('\\t') for l in lines]"
      ],
      "metadata": {
        "id": "BJxibfm7hR6o"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(eng), len(spa)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lbzCtMRzmKom",
        "outputId": "37cc774a-8f81-46cd-c837-f354e143bc88"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(118964, 118964)"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_pairs[10:40:4]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MS0DIQN0nQfm",
        "outputId": "cb6d317a-63c4-4894-8024-0f0de31822aa"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Fire!', '¡Disparad!'],\n",
              " ['Jump!', '¡Salta!'],\n",
              " ['Stop!', '¡Pare!'],\n",
              " ['Go on.', 'Continúe.'],\n",
              " ['I try.', 'Lo intento.'],\n",
              " ['Smile.', 'Sonríe.'],\n",
              " ['Go now.', 'Ve ahora mismo.'],\n",
              " ['He ran.', 'Él corrió.']]"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget  http://www.manythings.org/anki/fra-eng.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vSbsPYwx2gx6",
        "outputId": "33755294-84a6-4348-c001-dd6d44b97cb6"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-01-16 17:41:50--  http://www.manythings.org/anki/fra-eng.zip\n",
            "Resolving www.manythings.org (www.manythings.org)... 173.254.30.110\n",
            "Connecting to www.manythings.org (www.manythings.org)|173.254.30.110|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6720195 (6.4M) [application/zip]\n",
            "Saving to: ‘fra-eng.zip’\n",
            "\n",
            "fra-eng.zip         100%[===================>]   6.41M  31.7MB/s    in 0.2s    \n",
            "\n",
            "2023-01-16 17:41:50 (31.7 MB/s) - ‘fra-eng.zip’ saved [6720195/6720195]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip --qq '/content/fra-eng.zip'"
      ],
      "metadata": {
        "id": "lqdxo6vy26CK"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/fra.txt') as f:\n",
        "  f_file = f.read()\n",
        "\n",
        "f_lines = f_file.splitlines()\n",
        "text_pairs = [l.split('\\t')[:2] for l in f_lines]"
      ],
      "metadata": {
        "id": "RUNg_yG8rOJd"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_pairs[9]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRQC1UmPrOM0",
        "outputId": "23bcb927-441a-453d-c5bc-7d1838b70802"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Run!', 'File !']"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, let's split the sentence pairs into a training set, a validation set, and a test set.\n",
        "random.shuffle(text_pairs)\n",
        "num_val_samples = int(0.15 * len(text_pairs))\n",
        "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
        "train_pairs = text_pairs[:num_train_samples]\n",
        "val_pairs = text_pairs[num_train_samples : num_train_samples + num_val_samples]\n",
        "test_pairs = text_pairs[num_train_samples + num_val_samples :]\n",
        "\n",
        "print(f\"{len(text_pairs)} total pairs\")\n",
        "print(f\"{len(train_pairs)} training pairs\")\n",
        "print(f\"{len(val_pairs)} validation pairs\")\n",
        "print(f\"{len(test_pairs)} test pairs\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5W_wMwEj2x9j",
        "outputId": "38adaea1-1550-4ad4-9025-c5bb5ab51ea9"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "197463 total pairs\n",
            "138225 training pairs\n",
            "29619 validation pairs\n",
            "29619 test pairs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing the text data\n",
        "#textVectorization layer.\n",
        "strip_chars = string.punctuation + \"¿\"\n",
        "strip_chars = strip_chars.replace(\"[\", \"\")\n",
        "strip_chars = strip_chars.replace(\"]\", \"\")\n",
        "\n",
        "vocab_size, sequence_length, batch_size = 15000, 20, 96\n",
        "\n",
        "def custom_standardization(input_string):\n",
        "    lowercase = tf.strings.lower(input_string)\n",
        "    return tf.strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")\n",
        "\n",
        "eng_vectorization = TextVectorization(\n",
        "    max_tokens= vocab_size, output_mode=\"int\", output_sequence_length=20,)\n",
        "fra_vectorization = TextVectorization(max_tokens= 30000, output_mode=\"int\",\n",
        "    output_sequence_length = 20 + 1, standardize = custom_standardization,)\n",
        "\n",
        "train_eng_texts = [pair[0] for pair in train_pairs]\n",
        "train_fra_texts = [pair[1] for pair in train_pairs]\n",
        "eng_vectorization.adapt(train_eng_texts)\n",
        "fra_vectorization.adapt(train_fra_texts)"
      ],
      "metadata": {
        "id": "vi3jx0Uq2yBE"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking french vocabulary size\n",
        "fra_vect = TextVectorization(max_tokens= None, output_mode=\"int\",\n",
        "    output_sequence_length = 20 + 1, standardize = custom_standardization,)\n",
        "fra_vect.adapt(train_fra_texts)\n",
        "len(fra_vect.get_vocabulary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4tw1HQ2uzt-1",
        "outputId": "0488ed70-06d4-4c40-cf12-d20f069df58a"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "31110"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(eng_vectorization.get_vocabulary()), len(fra_vectorization.get_vocabulary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Qg3wZ_NAphw",
        "outputId": "7e2c1936-d87a-4294-9b18-f81731598e9f"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(14285, 15000)"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def format_dataset(eng, fra):\n",
        "    eng = eng_vectorization(eng)\n",
        "    fra = fra_vectorization(fra)\n",
        "    return ({\"encoder_inputs\": eng, \"decoder_inputs\": fra[:, :-1],}, fra[:, 1:])\n",
        "\n",
        "def make_dataset(pairs):\n",
        "    eng_texts, fra_texts = zip(*pairs)\n",
        "    eng_texts = list(eng_texts)\n",
        "    fra_texts = list(fra_texts)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, fra_texts))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.map(format_dataset)\n",
        "    return dataset.shuffle(2048).prefetch(16).cache()\n",
        "\n",
        "train_ds = make_dataset(train_pairs)\n",
        "val_ds = make_dataset(val_pairs)"
      ],
      "metadata": {
        "id": "K5iHGqru4ZoJ"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Making a PositionalEmbedding layer Class\n",
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
        "        super(PositionalEmbedding, self).__init__(**kwargs)\n",
        "        self.token_embeddings = layers.Embedding( input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.position_embeddings = layers.Embedding( input_dim=sequence_length, output_dim=embed_dim)\n",
        "        self.sequence_length = sequence_length\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return tf.math.not_equal(inputs, 0)"
      ],
      "metadata": {
        "id": "nsSqmIvt11uK"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# THIRD MODEL = \n",
        "embed_dim = 256\n",
        "latent_dim = 2048\n",
        "num_heads = 8\n",
        "\n",
        "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
        "encoder_outputs = keras_nlp.layers.TransformerEncoder( latent_dim, num_heads)(x)\n",
        "encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
        "\n",
        "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n",
        "encoded_seq_inputs = keras.Input(shape=(None, embed_dim), name=\"decoder_state_inputs\")\n",
        "x = PositionalEmbedding(sequence_length, 30000, embed_dim)(decoder_inputs)\n",
        "x = keras_nlp.layers.TransformerDecoder( latent_dim, num_heads)(x, encoded_seq_inputs)\n",
        "x = layers.Dropout(0.3)(x)\n",
        "decoder_outputs = layers.Dense(30000, activation=\"softmax\")(x)\n",
        "decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n",
        "\n",
        "decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
        "transformer = keras.Model( [encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\")"
      ],
      "metadata": {
        "id": "KeiZaDsF1Rgv"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZR8yXjxt_fk",
        "outputId": "8c611bcb-510a-4511-b389-026bd3295d97"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"transformer\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " encoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " positional_embedding (Position  (None, None, 256)   4101120     ['encoder_inputs[0][0]']         \n",
            " alEmbedding)                                                                                     \n",
            "                                                                                                  \n",
            " decoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " transformer_encoder (Transform  (None, None, 256)   1315072     ['positional_embedding[0][0]']   \n",
            " erEncoder)                                                                                       \n",
            "                                                                                                  \n",
            " model_1 (Functional)           (None, None, 16000)  10711168    ['decoder_inputs[0][0]',         \n",
            "                                                                  'transformer_encoder[0][0]']    \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 16,127,360\n",
            "Trainable params: 16,127,360\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#transformer.summary()\n",
        "transformer.compile( \"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "transformer.fit(train_ds, epochs=2, validation_data=val_ds,)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qL9CjqXG1RkB",
        "outputId": "c2154a00-0b5b-4157-ca37-3cc2948c7957"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "1440/1440 [==============================] - 155s 105ms/step - loss: 0.7827 - accuracy: 0.6487 - val_loss: 0.7687 - val_accuracy: 0.6447\n",
            "Epoch 2/2\n",
            "1440/1440 [==============================] - 153s 106ms/step - loss: 0.7391 - accuracy: 0.6727 - val_loss: 0.7644 - val_accuracy: 0.6502\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f47d8c890d0>"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "fra_vocab = fra_vectorization.get_vocabulary()\n",
        "fra_index_lookup = dict(zip(range(len(fra_vocab)), fra_vocab))\n",
        "max_decoded_sentence_length = 20\n",
        "\n",
        "def decode_sequence(input_sentence):\n",
        "    tokenized_input_sentence = eng_vectorization([input_sentence])\n",
        "    decoded_sentence = \"[start]\"\n",
        "    for i in range(max_decoded_sentence_length):\n",
        "        tokenized_target_sentence = fra_vectorization([decoded_sentence])[:, :-1]\n",
        "        predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n",
        "\n",
        "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
        "        sampled_token = fra_index_lookup[sampled_token_index]\n",
        "        decoded_sentence += \" \" + sampled_token\n",
        "\n",
        "        if sampled_token == \"[end]\":\n",
        "            break\n",
        "    return decoded_sentence"
      ],
      "metadata": {
        "id": "6eOpBvMe-uC3"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eng_texts = [l[0] for l in text_pairs]\n",
        "\n",
        "for _ in range(10):\n",
        "  input_sent = random.choice(eng_texts)\n",
        "  translated = decode_sequence(input_sent)\n",
        "  translate = translated.replace('[start]','')\n",
        "  print( input_sent,':',translate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMm2DVLIv2tX",
        "outputId": "d2ff8f74-269b-47c7-aac1-c3e773d2ddfe"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Is this Tom's car? :  cette voiture  de                \n",
            "Tom forgot to pay his rent. :  a oublié de payer son ici              \n",
            "Don't even think about it. :  pas même à ce sujet               \n",
            "They all objected to his proposal. :  toute sa proposition                 \n",
            "I want to go back to doing what I was doing before you interrupted me. :  je veux aller à faire ce que je voulais que vous me voulez que je me faire un coup je\n",
            "He drowned in the river. :  dans la rivière                 \n",
            "Will you please come with me? :  je vous prie                 \n",
            "Did you hear what we said? :  ce que nous avons dit               \n",
            "I swore I'd never do that. :  je navais jamais fait ça               \n",
            "Did you take a shower today? :  une douche aujourdhui                 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X9_XG5jDv2wv"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1eDA4hP7XZk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "06Cb7it1Xxj5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}